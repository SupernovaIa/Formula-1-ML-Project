{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing  \n",
    "# -----------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Path configuration for custom module imports  \n",
    "# -----------------------------------------------------------------------\n",
    "import sys  \n",
    "sys.path.append('../')  # Adds the parent directory to the path for custom module imports  \n",
    "\n",
    "# Custom functions and classes\n",
    "# -----------------------------------------------------------------------\n",
    "from src.preprocess import *\n",
    "from src.race_prediction_model.classification import ClassificationModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/output/featured_results.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DriverId', 'TeamId', 'Position', 'GridPosition', 'Time', 'Status',\n",
       "       'Points', 'round', 'circuitId', 'Winner', 'Podium', 'MeanPreviousGrid',\n",
       "       'MeanPreviousPosition', 'CurrentDriverWins', 'CurrentDriverPodiums'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to predict whether a driver will win a race, we need to remove the columns that contain information about the race result, as we cannot provide input data about something that has not yet happened.\n",
    "\n",
    "Our target variable in this case is `Winner` (it could be `Podium` if we want to predict if a driver will finish on the podium, `Position` if we want to predict the exact position, etc.).\n",
    "\n",
    "Therefore, we can remove `Position`, `Time`, `Status`, `Points`, `Podium`. The rest of the variables can be known before the race takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Winner'\n",
    "\n",
    "drop = 'Winner' if target == 'Podium' else 'Podium'\n",
    "\n",
    "df.drop(columns=['Position', 'Time', 'Status', 'Points', drop], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6433 entries, 8 to 11\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   DriverId              6433 non-null   object \n",
      " 1   TeamId                6433 non-null   object \n",
      " 2   GridPosition          6433 non-null   float64\n",
      " 3   round                 6433 non-null   int64  \n",
      " 4   circuitId             6433 non-null   object \n",
      " 5   Winner                6433 non-null   int64  \n",
      " 6   MeanPreviousGrid      6433 non-null   float64\n",
      " 7   MeanPreviousPosition  6433 non-null   float64\n",
      " 8   CurrentDriverWins     6433 non-null   int64  \n",
      " 9   CurrentDriverPodiums  6433 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(3)\n",
      "memory usage: 552.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explicitly check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DriverId                0\n",
       "TeamId                  0\n",
       "GridPosition            0\n",
       "round                   0\n",
       "circuitId               0\n",
       "Winner                  0\n",
       "MeanPreviousGrid        0\n",
       "MeanPreviousPosition    0\n",
       "CurrentDriverWins       0\n",
       "CurrentDriverPodiums    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DriverId', 'TeamId', 'circuitId'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include='O').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "We only need to encode the columns `DriverId`, `TeamId`, and `circuitId`.\n",
    "\n",
    "* `DriverId`: We will apply target encoding since we want to give more weight to drivers with more victories.\n",
    "\n",
    "* `TeamId`: We will apply target encoding since we want to give more weight to teams with more victories.\n",
    "\n",
    "* `circuitId`: We will use ordinal encoding, as the circuits don't have any significance beyond the fact that some teams or drivers perform better than others.\n",
    "\n",
    "### Scaling\n",
    "\n",
    "Since we have very few outliers in our datasets and there are no extremely high values, we will use a `MinMax` scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_methods = {\"onehot\": [],\n",
    "                    \"target\": ['DriverId', 'TeamId'],\n",
    "                    \"ordinal\" : {\n",
    "                        'circuitId': df['circuitId'].unique().tolist()\n",
    "                        },\n",
    "                    \"frequency\": []\n",
    "                    }\n",
    "scaling = 'minmax'\n",
    "\n",
    "df_encoded, df_scaled = preprocess(df, encoding_methods, scaling, target_variable=target, save_objects=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are facing a binary classification problem in which we aim to predict whether a driver will win a race or not based on various data. Therefore, we will test the simplest classification algorithm, `logistic regression`, alongside a more sophisticated model like `XGBoost`.\n",
    "\n",
    "Regarding the model metrics, `precision` is the key metric, as it is more important for us to ensure that if we predict a driver will win, they actually do, even at the cost of sometimes missing drivers who will win. However, we will also consider `f1_score`, which provides a balance between both situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we instantiate the `ClassificationModels` class with the scaled dataset and the target variable to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ClassificationModels(df_scaled, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an empty dataframe where we will store the metric results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"logistic_regression\"\n",
    "\n",
    "# Fit model\n",
    "models.fit_model(model, file_name=model, cross_validation=10)\n",
    "\n",
    "# Get metrics and store them\n",
    "df_current_results = models.get_metrics(model)\n",
    "df_current_results[\"model\"] = model\n",
    "df_results = pd.concat([df_results, df_current_results], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>kappa</th>\n",
       "      <th>auc</th>\n",
       "      <th>average_precision</th>\n",
       "      <th>time_seconds</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.963</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.643</td>\n",
       "      <td>2.196</td>\n",
       "      <td>logistic_regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.960</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.490</td>\n",
       "      <td>2.196</td>\n",
       "      <td>logistic_regression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy  precision  recall     f1  kappa    auc  average_precision  \\\n",
       "train     0.963      0.692   0.465  0.556  0.538  0.965              0.643   \n",
       "test      0.960      0.474   0.367  0.414  0.394  0.954              0.490   \n",
       "\n",
       "       time_seconds                model  \n",
       "train         2.196  logistic_regression  \n",
       "test          2.196  logistic_regression  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_current_results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Logistic Regression` model achieves a high accuracy of 96% and an AUC-ROC of 0.95, indicating strong class discrimination.\n",
    "\n",
    "However, performance drops when evaluating the test set, with a decrease in precision (from 0.69 to 0.47) and recall (from 0.46 to 0.37). This suggests potential overfitting to the training data, reducing the model's generalization ability.\n",
    "\n",
    "Additionally, these metric values are not particularly strong, so we will aim to improve them using XGBoost and class balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.plot_confusion_matrix(model, size=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `confusion matrix` for `Logistic Regression` shows that the model performs well on the negative class (0) but struggles with the positive class (1):\n",
    "\n",
    "- True negatives (TN): 1218 → Correctly classified as 0.\n",
    "- False positives (FP): 20 → Incorrectly classified as 1.\n",
    "- False negatives (FN): 31 → Cases of class 1 misclassified as 0.\n",
    "- True positives (TP): 18 → Correctly classified as 1.\n",
    "\n",
    "The model has high precision for class 0, but its recall for class 1 is low, capturing only 18 out of 49 positive cases (≈36.7%). This indicates difficulty in correctly identifying the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.plot_predictors_importance(model, size=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding feature importance, we observe results that make sense. In absolute terms, the most significant factors are `starting grid position` and `average position in the previous races` (specifically, the last 3 races in this case).\n",
    "\n",
    "Additionally, we see that `the number of current wins` and the `driver ID` are important features. It's worth noting that the driver ID was encoded using `target encoding`, which explains why drivers with the most wins are more likely to be predicted as winners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.plot_shap_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `SHAP diagram`, we can better visualize the impact of each feature on the model, further reinforcing our previous conclusions.  \n",
    "\n",
    "Interestingly, while the most influential feature is the starting grid position for the current race, the previous grid positions appear to have little to no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"xgboost\"\n",
    "\n",
    "# Fit model\n",
    "models.fit_model(model, file_name=model, cross_validation=10)\n",
    "\n",
    "# Get metrics and store them\n",
    "df_current_results = models.get_metrics(model)\n",
    "df_current_results[\"model\"] = model\n",
    "df_results = pd.concat([df_results, df_current_results], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current_results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `XGBoost model` achieves a high accuracy of 97.4% and an AUC-ROC of 0.967, indicating excellent class discrimination, slightly improving over `Logistic Regression` in both metrics.  \n",
    "\n",
    "Compared to Logistic Regression, XGBoost exhibits better precision on the test set (0.75 vs. 0.47), meaning it makes fewer false positive predictions. However, recall drops slightly indicating that the model still struggles to identify all positive cases, but it's still better than Logistic Regression (0.49 vs. 0.37). The F1-score (0.593) shows an improvement over Logistic Regression as well, demonstrating a more balanced trade-off between precision and recall.  \n",
    "\n",
    "While overfitting is still present, XGBoost provides a more **robust performance overall**. To further enhance results, we could explore hyperparameter tuning and class balancing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.plot_confusion_matrix(model, size=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `confusion matrix` for `XGBoost` shows that the model performs well in classifying the negative class (0)** but still struggles with the positive class (1):\n",
    "\n",
    "- True negatives (TN): 1230 → Correctly classified as 0.  \n",
    "- False positives (FP): 8 → Incorrectly classified as 1.  \n",
    "- False negatives (FN): 25 → Class 1 cases misclassified as 0.  \n",
    "- True positives (TP): 24 → Correctly classified as 1.  \n",
    "\n",
    "Compared to Logistic Regression, XGBoost significantly reduces false positives (from 20 to 8), improving precision. However, it still struggles with recall, as it only captures 24 out of 49 actual positive cases (≈49%). While this is an improvement over Logistic Regression, it suggests that the model still misses many true winners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.plot_predictors_importance(model, size=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.plot_shap_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we observe that `previous finishing positions` have greater importance than the `starting grid position` for the current race.\n",
    "\n",
    "Additionally, the `starting grid positions from previous races` and the `circuit` now carry more weight compared to the previous model, whereas current podiums and victories have less influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Class balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance  \n",
    "\n",
    "One of the main challenges we face is **class imbalance**. This is expected, as a Formula 1 race typically features 20 drivers, but only one wins, meaning the minority class represents just 5% of the dataset.  \n",
    "\n",
    "A common approach to handling this issue is upsampling or downsampling. However, these techniques often introduce biases into the dataset. Instead, we will use a different method: reducing the dataset to include only relevant entries for the model.  \n",
    "\n",
    "Many drivers have a very low probability of winning due to various factors. Therefore, we can exclude them from the dataset to improve model performance.  \n",
    "\n",
    "### Filtering criterion\n",
    "\n",
    "We will retain only drivers whose starting and finishing positions are below a certain threshold. This removes participants who consistently qualify and finish in poor positions, as they realistically never had a chance of winning. However, drivers who:  \n",
    "\n",
    "- Started in a poor position but managed a strong finish will still be included.  \n",
    "- Started in a good position but had a bad race will also remain in the dataset.  \n",
    "\n",
    "This filtering process only excludes drivers who performed poorly in both sessions (qualifying and race), ensuring we keep those who are genuinely competitive in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/output/featured_results.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply the filter. After several tests, we have decided to use a criterion that includes only results where the driver started or finished within the top three positions.\n",
    "\n",
    "This approach ensures that we retain only the most competitive results while removing those who had little to no realistic chance of winning. By doing so, we aim to improve the model's performance by focusing on relevant cases and reducing the impact of class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 3 positions\n",
    "threshold = 3\n",
    "mask = (df['Position'] <= threshold) | (df['GridPosition'] <= threshold)\n",
    "\n",
    "# Apply mask\n",
    "df = df[mask]\n",
    "\n",
    "target = 'Winner'\n",
    "drop = 'Winner' if target == 'Podium' else 'Podium'\n",
    "\n",
    "df.drop(columns=['Position', 'Time', 'Status', 'Points', drop], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We carry out the same preprocessing as in the previous case to ensure consistency in the data preparation and maintain comparability between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_methods = {\"onehot\": [],\n",
    "                    \"target\": ['DriverId', 'TeamId'],\n",
    "                    \"ordinal\" : {\n",
    "                        'circuitId': df['circuitId'].unique().tolist()\n",
    "                        },\n",
    "                    \"frequency\": []\n",
    "                    }\n",
    "scaling = 'minmax'\n",
    "\n",
    "df_encoded, df_scaled = preprocess(df, encoding_methods, scaling, target_variable=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Training and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_balanced = ClassificationModels(df_scaled, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_balanced = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\"logistic_regression\", \"xgboost\"]\n",
    "\n",
    "for model in model_list:    \n",
    "\n",
    "    # Fit model\n",
    "    models_balanced.fit_model(model, file_name=f\"{model}_balanced\", cross_validation=10)\n",
    "\n",
    "    # Plots\n",
    "    models_balanced.plot_confusion_matrix(model, size=(6,5))\n",
    "    models_balanced.plot_shap_summary(model)\n",
    "\n",
    "    # Get metrics\n",
    "    df_current_results = models_balanced.get_metrics(model)\n",
    "    df_current_results[\"model\"] = model\n",
    "    df_results_balanced = pd.concat([df_results_balanced, df_current_results], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_balanced.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data balancing has shown a significant impact on the metrics of the logistic regression model, reducing the bias caused by the original imbalance and improving its generalization ability. In this case, correcting the distribution has allowed the model to better capture the patterns of the minority classes, reducing the tendency to predominantly predict the majority class. As a result, the imbalance issue has practically disappeared in this model.\n",
    "\n",
    "On the other hand, in XGBoost, balancing has not generated significant improvements and, in some cases, has worsened the metrics. This is because XGBoost, being a tree-based model with internal mechanisms for handling imbalances (such as weight adjustments in the loss function and differential error assignment), is less affected by imbalanced distributions. Additionally, the balanced dataset has introduced slight overfitting in this model, suggesting that the adjustment to the new distribution has led to greater sensitivity to patterns specific to the training set, rather than an improvement in its generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_results.round(3))\n",
    "display(df_results_balanced.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "- Full dataset:\n",
    "  - Train: Accuracy = 0.963, F1 = 0.556, AUC = 0.965\n",
    "  \n",
    "  - Test: Accuracy = 0.960, F1 = 0.414, AUC = 0.954\n",
    "\n",
    "  - Train-test difference: Some overfitting.\n",
    "\n",
    "- Balanced dataset:\n",
    "  - Train: Accuracy = 0.828, F1 = 0.533, AUC = 0.831\n",
    "\n",
    "  - Test: Accuracy = 0.782, F1 = 0.558, AUC = 0.790\n",
    "\n",
    "  - Train-test difference: Better generalization than the full dataset.\n",
    "\n",
    "  - Improved recall and F1 in test, meaning the model now predicts the minority class better.\n",
    "\n",
    "### XGBoost\n",
    "- Full dataset:\n",
    "  - Train: Accuracy = 0.971, F1 = 0.569, AUC = 0.981\n",
    "\n",
    "  - Test: Accuracy = 0.974, F1 = 0.490, AUC = 0.967\n",
    "\n",
    "  - Train-test difference: Small, good generalization, but slight overfitting.\n",
    "\n",
    "- Balanced dataset:\n",
    "  - Train: Accuracy = 0.893, F1 = 0.713, AUC = 0.945\n",
    "\n",
    "  - Test: Accuracy = 0.782, F1 = 0.584, AUC = 0.808\n",
    "\n",
    "  - Train-test difference: Larger, indicating clear overfitting.\n",
    "\n",
    "#### Final decision: Best model selection\n",
    "\n",
    "| Model | Generalization | Overfitting | Performance (F1, AUC) | Training time |\n",
    "|--------|---------------|-------------|------------------------|-----------------|\n",
    "| `Logistic Regression (Balanced)` | Good | Low | Better recall and F1 in test | Fast |\n",
    "| `XGBoost (Full dataset)` | Good | Slight | Best in AUC and precision | Moderate |\n",
    "| `XGBoost (Balanced)` | Worse | High | Good F1 but overfitting | Moderate |\n",
    "\n",
    "#### Final choice: `XGBoost with the full dataset`\n",
    "- If training time is not an issue, `XGBoost with the full dataset` provides the best overall performance.\n",
    "\n",
    "- If computational efficiency is critical, `logistic regression with the balanced dataset` is a solid alternative.\n",
    "\n",
    "**Conclusion: XGBoost (full dataset) for best performance, Logistic Regression (balanced) for efficiency.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model file saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.fit_model('xgboost', file_name='best_model', cross_validation=10)\n",
    "models.get_metrics('xgboost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinalProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
